{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Interpretable Machine Learning: Shapley Values\n",
    "#### United Lunch & Learn: June 6, 2019\n",
    "\n",
    "_Author: Carleton Smith_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Outline\n",
    "\n",
    "- Install/import packages\n",
    "- Acquire data\n",
    "- Initial EDA\n",
    "- Prepare dataset\n",
    "- Overview of Shapley Values\n",
    "    - One\n",
    "    - Two\n",
    "    - Three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install -yc conda-forge --prefix {sys.prefix} shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Introduction\n",
    "\n",
    "Before demonstrating how to use Shapley Values for machine learning, let's discuss what they are in the first place. This explanation is based on these two papers:\n",
    "\n",
    "1. [A Unified Approach to Interpreting Model Predictions](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf)\n",
    "2. [Consistent Individualized Feature Attribution for Tree Ensembles](https://arxiv.org/pdf/1802.03888.pdf)\n",
    "\n",
    "#### TL;DR\n",
    "\n",
    "Shapley Values originated in game theory and are named after famed mathematician and Nobel Prize winner, [Lloyd Shapley](https://en.wikipedia.org/wiki/Lloyd_Shapley). The purpose of Shapley Values from a game theory perspective is to solve the problem of assigning appropriate credit to individual players in a cooperative game. In recent years, researchers have adopted Shapley Values to assign \"credit\" to features for predictions produced by a complex model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Why do we care about interpretable machine learning?\n",
    "\n",
    "There are many reasons.\n",
    "\n",
    "- Establish trust in the model\n",
    "- Better understand underlying processes\n",
    "- Provide insights in how to improve the model\n",
    "- Ethical and fairness risks\n",
    "\n",
    "In addition to Shapley Values, several methods exist on the market for this purpose:\n",
    "\n",
    "- LIME\n",
    "- DeepLIFT\n",
    "- Layer-Wise Relevance Propagation\n",
    "\n",
    "What sets Shapley Values apart (according to the [the first paper listed above](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf)) is that Shapely values are the only one of these method that satisfy all three of these feature importance quality requirements:\n",
    "\n",
    "1. **Local Accuracy** - a simple model explaining a complex one around a particular point should produce the same output given the same inputs\n",
    "2. **Missingness** - if a feature is missing in the input space, it should not appear in the feature attribution\n",
    "3. **Consistency** - if a feature is increasing in it's contribution to the outcome, then that feature should increase in it's importance\n",
    "\n",
    "What ends up happening with many of the other methods is that you can come up with counter-examples where you cannot acheive all 3 of these properties at the same time. Shapley Values are the only solution among these that do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Theory Context Example: A Soccer Team\n",
    "\n",
    "Original problem proposed in game theory was that sometimes you have situations where a group of people are working together to acheive an outcome. A good example of this is a soccer team.\n",
    "\n",
    "_Soccer Team_:\n",
    "- 11 players\n",
    "- Well forumated positions\n",
    "\n",
    "How important are each one of those 11 players to the overall performance of the team? There's just one outcome, but 11 people contributing.\n",
    "\n",
    "Imagine you are running an experiment with a whole stadium of soccer players and you are creating differet coalitons of soccer players and composing teams of them. Sometimes you have teams of 1 person.\n",
    "\n",
    "Let's assume the scores are more continuous - 1 and 100 range.\n",
    "\n",
    "1 player might be really good, but by themselves, not a good team. Perhaps by themself score a 10.\n",
    "Then you add a player, and that person is also really good, so they are a 9 by themself. But instead of linearly adding them together, the synergistic affect would be a 21 or 22. Then maybe you have a 3rd person, who is a 5, but plays well with the first two, so they add in +8. Then you add in a fourth that is a 7 by themselves, but they get into fights with the first two, so they only add +2 when you add them in. Then you add a 5th one. And this one is a goalie, so this is a position that is totally different than any others, so that might be very valuable. Then you add a 6th one, and they're also a goalie, so that's not as valuable. If they were the first goalie, then it would be valuable, but the order of being added matters. You're trying to understand the coalition that you're creating here, but it dependes on the diff combinations of people and the order in which they enter the game. Since this depends on the unique cominations of individuals and the order in which they are added, the credit attribution problem becomes faily complex.\n",
    "\n",
    "The question is, is there some way we can summarize value? At the end of the day, there may just be people that are consistently higher impact or lower impact if you were to add up all the possible combinations coalitions.\n",
    "\n",
    "This is a very large multiplication For each one of the scenarios that you construct, you pay attention to what changes in the overall credit that the team gets when you add in a particular actor to this coalition. Add in all of those contributions up by each player, then divide by number of scenarios you have and that's the Shapley Value for that associated with that person. Fairly straightforward process to calculate, but is very computationally expensive.\n",
    "\n",
    "When you have potentially large compositions of coalitions, then you have to go through the exercise of knowing what each of those credit assignments are.\n",
    "\n",
    "Tie back to ML: Instead of the soccer score, we care about a model's prediction of a particular case. Instead of soccer players, we are concerned with what features are playing the biggest role in making this prediction. A Shapley Value tells you how important each feature is for a particular prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Acquire Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\n",
    "                    na_values=' ?',\n",
    "                    header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above line hangs, then uncomment the line below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adult = pd.read_csv('./datasets/adult.data.txt', header=None, na_values=' ?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Preprocessing/EDA\n",
    "\n",
    "- Add column headers\n",
    "- Understand dataset\n",
    "    - how many rows/columns?\n",
    "    - what does a row represent?\n",
    "    - what is our target variable?\n",
    "- Check for missing values\n",
    "- Check data types\n",
    "- Check for unbalanced target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Column Headers\n",
    "\n",
    "**FEATURES**\n",
    "\n",
    "1. `age`: continuous.\n",
    "2. `workclass`: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "3. `fnlwgt`: continuous.\n",
    "4. `education`: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "5. `education-num`: continuous.\n",
    "6. `marital-status`: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "7. `occupation`: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "8. `race`: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "9. `sex`: Female, Male.\n",
    "10. `capital-gain`: continuous.\n",
    "11. `capital-loss`: continuousm\n",
    "12. `hours-per-week`: continuous.\n",
    "13. `native-country`: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education_num',\n",
    "    'marital_status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital_gain',\n",
    "    'capital_loss',\n",
    "    'hours_per_week',\n",
    "    'native_country',\n",
    "    'income',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign column names\n",
    "adult.columns = features\n",
    "adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many rows a columns?\n",
    "adult.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any missing values?\n",
    "adult.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the data types?\n",
    "adult.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the distribution of our target variable?\n",
    "adult['income'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preprocessing\n",
    "\n",
    "In the interest of time, I packaged these preprocessing steps into `Pipelines`.\n",
    "\n",
    "**PREPROCESSING STEPS**\n",
    "1. Separate target variable from features - sklearn requires this.\n",
    "2. Peform a train-test split - Always do this before manipulating dataset\n",
    "3. With training data:\n",
    "    - **SEPARATE** numeric columns from categorical ones\n",
    "    - **NUMERIC DF** preprocessing:\n",
    "        - Replace nan values\n",
    "        - Standardize features\n",
    "   \n",
    "    - **CATEGORICAL DF** preprocessing:\n",
    "        - Replace nan values\n",
    "        - Create dummy variables\n",
    "    - **CONCATENATE** numeric and categorical DF\n",
    "    - **ENCODE** target variable\n",
    "<br>\n",
    "<br>\n",
    "4. Package these steps into a `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of numeric and categorical column names\n",
    "num_cols = [col for col in adult.columns if adult[col].dtype != 'object']\n",
    "cat_cols = [col for col in adult.columns if col not in num_cols + ['income']]\n",
    "\n",
    "# separate features from target variable\n",
    "X = adult.drop('income', axis=1)\n",
    "y = adult['income']\n",
    "\n",
    "# perform a train-test split.... why? stratify on y?\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    stratify=y,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "def feature_extractor(df):\n",
    "    return df.drop('income', axis=1)\n",
    "\n",
    "\n",
    "def categorical_extractor(df):\n",
    "    return df.select_dtypes(include=['object'])\n",
    "\n",
    "\n",
    "def numeric_extractor(df):\n",
    "    return df.select_dtypes(exclude=['object'])\n",
    "\n",
    "# create custom transformers\n",
    "cat_transformer = FunctionTransformer(categorical_extractor, validate=False)\n",
    "num_transformer = FunctionTransformer(numeric_extractor, validate=False)\n",
    "\n",
    "# make numeric pipe\n",
    "num_pipe = Pipeline([\n",
    "    ('numeric_transformer', num_transformer),\n",
    "    ('num_im', SimpleImputer(strategy='median')),\n",
    "    ('StandardScaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# make categorical pipe\n",
    "cat_pipe = Pipeline([\n",
    "    ('cat_transformer', cat_transformer),\n",
    "    ('cat_im', SimpleImputer(strategy='most_frequent')),\n",
    "    ('OrdinalEncoder', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "\n",
    "# make FeatureUnion\n",
    "feat_union = FeatureUnion([\n",
    "    ('num_pipe', num_pipe),\n",
    "    ('cat_pipe', cat_pipe)\n",
    "])\n",
    "\n",
    "# make final feature pipe\n",
    "feature_pipe = Pipeline([\n",
    "    ('feat_union', feat_union)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use this pipeline to _fit_ and _transform_ `X_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform training data\n",
    "X_train_prepared = pd.DataFrame(\n",
    "    feature_pipe.fit(X_train).transform(X_train),\n",
    "    index=X_train.index,\n",
    "    columns=X_train.columns)\n",
    "X_train_prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform testing data\n",
    "X_test_prepared = pd.DataFrame(\n",
    "    feature_pipe.transform(X_test),\n",
    "    index=X_test.index,\n",
    "    columns=X_test.columns)\n",
    "X_test_prepared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `LabelEncoder` to transform the `income` to be numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform y_train\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = pd.Series(le.fit_transform(y_train), index=y_train.index)\n",
    "y_train_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform y_test\n",
    "y_test_encoded = pd.Series(le.transform(y_test), index=y_test.index)\n",
    "y_test_encoded[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_encoded.value_counts()[0] / y_test_encoded.value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Explainable Model: `GradientBoostingClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=10,\n",
    "    class_weight='balanced',\n",
    "    oob_score=True\n",
    ")\n",
    "rf.fit(X_train_prepared, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for training and test:\n",
    "y_pred_train = rf.predict(X_train_prepared)\n",
    "y_pred_test = rf.predict(X_test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CLASSIFICATION METRICS FOR TRAINING: \\n')\n",
    "print(classification_report(y_train_encoded, y_pred_train))\n",
    "print('#########################################################\\n')\n",
    "\n",
    "print('CLASSIFICATION METRICS FOR TESTING: \\n')\n",
    "print(classification_report(y_test_encoded, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_encoded, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(rf, X_train_prepared, y_train_encoded, scoring='recall', cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Top 10 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_lst = list(zip(X_train_prepared.columns, rf.feature_importances_))\n",
    "feat_lst = sorted(feat_imp_lst, key=lambda x: x[1], reverse=True)\n",
    "for tup in feat_lst[:10]:\n",
    "    print(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley values\n",
    "\n",
    "The `shap` package includes a C++ optimized implementation for several popular Python tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X_train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scratch Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lst = {\n",
    "    \"n_estimators\": np.arange(10, 105, 10),\n",
    "    \"max_depth\": [None, 30, 10, 5],\n",
    "    \"class_weight\": ['balanced'],\n",
    "    \"oob_score\": [True],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RandomizedSearchCV(rf, param_distributions=param_lst, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.fit(X_train_prepared, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
